{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment has three sections: this first section introduces and implements a standard sctor-critic algorightm; the second section implements and compares the actor-critic algorithm with Semi-gradient TD on the experiment environment of Mountain Car."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actorâ€“critic Algorithms\n",
    "Actor-critic algorithms use the gradient of the value function to update the policy parameters. The general form of actor-critic is shown as follows.\n",
    "![alt text](algorithm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "#policy\n",
    "ACTIONS = [-1, 0, 1]\n",
    "#value function\n",
    "class ValueFunction:\n",
    "    def __init__(self, featuresize, steprate):\n",
    "        # weights for features\n",
    "        self.theta = np.random.rand(featuresize)\n",
    "        self.steprate = steprate\n",
    "    def value(self, phi):\n",
    "        value = 0.0\n",
    "        for i in range(0, len(phi)):\n",
    "            if phi[i] > 0:\n",
    "                value += self.theta[i]\n",
    "        return value\n",
    "    def update(self, gradient):\n",
    "        self.theta += self.steprate * gradient\n",
    "        return np.linalg.norm(self.steprate * gradient)\n",
    "class PolicyEstimator:\n",
    "    def __init__(self, featuresize, steprate, car):\n",
    "        with tf.variable_scope(\"Policy\"):\n",
    "            self.state = tf.placeholder(tf.int32, [], \"state\")\n",
    "            self.action = tf.placeholder(dtype=tf.int32, name=\"action\")\n",
    "            self.target = tf.placeholder(dtype=tf.float32, name=\"target\")\n",
    "            state_one_hot = car.getState()\n",
    "            #MLP\n",
    "            self.output_layer = tf.contrib.layers.fully_connected(\n",
    "                inputs = tf.expand_dims(state_one_hot, 0),\n",
    "                num_outputs = 3,\n",
    "                activation_fn = None,\n",
    "                weights_initializer = tf.zeros_initializer)\n",
    "            self.action_probs = tf.squeeze(tf.nn.softmax(self.output_layer))\n",
    "            self.sel_action_prob = tf.gather(self.action_probs, self.action)\n",
    "            # Loss\n",
    "            self.loss = -tf.log(self.sel_action_prob) * self.target\n",
    "            #Optimizer\n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate = steprate)\n",
    "            self.train_op = self.optimizer.minimize(self.loss, global_step = tf.contrib.framework.get_global_step())\n",
    "    def actions(self, state):\n",
    "        sess = tf.get_default_session()\n",
    "        return sess.run(self.action_probs, { self.state: state })\n",
    "    def update(self, state, target, action):\n",
    "        sess = tf.get_default_session()\n",
    "        feed_dict = { self.state: state, self.target: target, self.action: action  }\n",
    "        return sess.run([self.train_op, self.loss], feed_dict)\n",
    "    \n",
    "def ActorCritic(tile_size):\n",
    "    car = MountainCar(tile_size)\n",
    "    learningrate = 0.1\n",
    "    gamma = 0.8\n",
    "    positions = np.zeros(500)\n",
    "    valueFunction = ValueFunction(tile_size * 10 * 10, learningrate)\n",
    "    policyEstimator = PolicyEstimator(tile_size * 10 * 10, learningrate, car)\n",
    "    for i in range(500):\n",
    "        state0 = car.getState()\n",
    "        actions = policyEstimator.actions(state0)\n",
    "        action = np.random.choice(np.arange(len(actions)), p=actions)\n",
    "        reward, position = car.takeAction(ACTIONS[action])\n",
    "        state1 = car.getState()\n",
    "        positions[i] = position\n",
    "        value_next = valueFunction.value(state1)\n",
    "        target = reward + gamma * value_next\n",
    "        error = target - valueFunction.predict(state0)\n",
    "        #update valuefuction\n",
    "        valueFunction.update(learningrate * td_error)\n",
    "        #update police estimater\n",
    "        policyEstimator.update(state, td_error, action)\n",
    "    return positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semi-gradient TD\n",
    "Update function,\n",
    "$ \\theta_{k+1} \\gets \\theta_{k} + \\alpha [R + \\gamma \\hat{v}(S^{'}, \\theta_{k}) - \\hat{v}(S, \\theta_{k})]\\phi $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define Semi-gradient TD\n",
    "def SemiTD(tile_size):\n",
    "    car = MountainCar(tile_size)\n",
    "    learningrate = 0.1\n",
    "    valueFunction = ValueFunction(tile_size * 10 * 10, learningrate)\n",
    "    gamma = 0.8\n",
    "    reward = 0.0\n",
    "    positions = np.zeros(500)\n",
    "    for i in range(500):\n",
    "        state0 = car.getState()\n",
    "        action = getAction(car, valueFunction)\n",
    "        reward, position = car.takeAction(action)\n",
    "        positions[i] = position\n",
    "        state1 = car.getState()\n",
    "        delta = gamma * valueFunction.value(state1) + reward - valueFunction.value(state0)\n",
    "        gradient = delta * state0\n",
    "        diff = valueFunction.update(gradient)\n",
    "        #RMSPBE\n",
    "        state1 = np.matrix(state1)\n",
    "        value = np.sqrt(np.dot(np.dot(delta * state1, np.transpose(state1)), \\\n",
    "                                     np.dot(state1, delta * np.transpose(state1))))\n",
    "    return positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment\n",
    "I evaluated different **gradient-based TD methods** on the classic RL environment **Mountain Car**[1]. Each run started near the bottom of the mountain (_-0.5_) with zero velocity. Three actions [_reverse, coast, forward_] are selected in each interation and the reward is always _-1_. The position and velocity are encoded by using 4 or 8 **tile coding** with 10\\*10 tilings.\n",
    "![alt text](MC.png \"Mountain Car\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "POSITION_MIN = -1.2\n",
    "POSITION_MAX = 0.6\n",
    "VELOCITY_MIN = -0.07\n",
    "VELOCITY_MAX = 0.07\n",
    "RUN = 100\n",
    "TILE_SIZE = 4\n",
    "# Tile Coding\n",
    "class TileCoding:\n",
    "    def __init__(self, tiling_num, tiling_size, tile_length, tile_width, space_length, space_width):\n",
    "        self.tiling_num = tiling_num\n",
    "        self.tiling_size = tiling_size\n",
    "        self.tile_length = tile_length\n",
    "        self.tile_width = tile_width\n",
    "        self.x_movement = -(tiling_size * tile_length - space_length)/tiling_num\n",
    "        self.y_movement = -(tiling_size * tile_width - space_width)/tiling_num\n",
    "    def genCode(self, x, y):\n",
    "        tileCode = [0] * self.tiling_num * self.tiling_size * self.tiling_size\n",
    "        original_x = POSITION_MIN\n",
    "        original_y = VELOCITY_MIN\n",
    "        for i in range(0, self.tiling_num):\n",
    "            grid_x = math.floor((x - original_x)/self.tile_length)\n",
    "            grid_y = math.floor((y - original_y)/self.tile_width)\n",
    "            #print int(grid_x * self.tiling_size + grid_y)\n",
    "            tileCode[int(grid_x * self.tiling_size + grid_y)] = 1\n",
    "            original_x += self.x_movement\n",
    "            original_y += self.y_movement\n",
    "        return tileCode\n",
    "# MountainCar with Tile Coding\n",
    "class MountainCar:\n",
    "    def __init__(self, tile_size):\n",
    "        self.position = -0.5\n",
    "        self.velocity = 0.0\n",
    "        self.tile = TileCoding(tile_size, 10, 0.25, 0.02, 1.8, 0.14)#4 tiles with 10*10 tilings\n",
    "    def resetState(self):\n",
    "        self.position = -0.5\n",
    "        self.velocity = 0.0\n",
    "    def isEnd(self):\n",
    "        if self.position >= 0.6:\n",
    "            return True\n",
    "        return False\n",
    "    def takeAction(self, action):\n",
    "        #action belongs to {1, 0, -1}\n",
    "        velocity = self.velocity + 0.001 * action - 0.0025 * np.cos(3 * self.position)\n",
    "        self.velocity = min(max(VELOCITY_MIN, velocity), VELOCITY_MAX)\n",
    "        position = self.position + self.velocity\n",
    "        self.position = min(max(POSITION_MIN, position), POSITION_MAX)\n",
    "        reward = -1.0\n",
    "        #if self.position == POSITION_MIN:\n",
    "        #    self.position = 0.0\n",
    "        return reward\n",
    "    def testAction(self, action):\n",
    "        #action belongs to {1, 0, -1}, test action\n",
    "        velocity = self.velocity + 0.001 * action - 0.0025 * np.cos(3 * self.position)\n",
    "        velocity = min(max(VELOCITY_MIN, velocity), VELOCITY_MAX)\n",
    "        position = self.position + velocity\n",
    "        position = min(max(POSITION_MIN, position), POSITION_MAX)\n",
    "        reward = -1.0\n",
    "        #if position == POSITION_MIN:\n",
    "        #    position = 0.0\n",
    "        return np.asarray(self.tile.genCode(position, velocity))\n",
    "    def getState(self):\n",
    "        #return state after tileCoding\n",
    "        return np.asarray(self.tile.genCode(self.position, self.velocity))\n",
    "tile_size = 8\n",
    "STD_positions = []\n",
    "for i in range(RUN):\n",
    "    STD_positions.append(SemiTD(tile_size))\n",
    "AC_positions = []\n",
    "for i in range(RUN):\n",
    "    AC_positions.append(ActorCritic(tile_size))\n",
    "plt.plot(np.mean(STD_positions, axis=0), label = 'Semi-gradient TD')\n",
    "plt.plot(np.mean(AC_positions, axis=0), label = 'ActorCritic')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Position')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "tile_size = 4\n",
    "STD_positions = []\n",
    "for i in range(RUN):\n",
    "    STD_positions.append(SemiTD(tile_size))\n",
    "AC_positions = []\n",
    "for i in range(RUN):\n",
    "    AC_positions.append(ActorCritic(tile_size))\n",
    "plt.plot(np.mean(STD_positions, axis=0), label = 'Semi-gradient TD')\n",
    "plt.plot(np.mean(AC_positions, axis=0), label = 'ActorCritic')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Position')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](car0.png \"SemiTD vs ActorCritic when size of tiles = 8\")\n",
    "![alt text](car1.png \"SemiTD vs ActorCritic when size of tiles = 4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I run experiment on a tensorflow environment separately. The following figure is ploted from the output of the methods of SemiTD and ActorCritic after 20 trials. When the size of tiles is 8, actor-critic performs more stable than SemiTD. However, When the size of tiles is 4, SemiTD performs more stable than actor-critic. It may be because when the size of tiles is 8, the policy estimator can have better estimation of actions probabilities "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "[1] Bhatnagar, S., Sutton, R., Ghavamzadeh, M., & Lee, M. (2009). Natural actor-critic algorithms. Automatica, 45(11).\n",
    "[2] Konda, V. R., & Tsitsiklis, J. N. (1999, November). Actor-Critic Algorithms. In NIPS (Vol. 13, pp. 1008-1014)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
